{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8124be-6650-46a6-befe-088036fd8a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    # HfArgumentParser,\n",
    "    # M2M100Tokenizer,\n",
    "    # M2M100Config,\n",
    "    # M2M100Model,\n",
    "    # M2M100ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    # default_data_collator,\n",
    "    # set_seed,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f4ba2-c7c9-4a4f-a68a-4621ca504f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\",src_lang=\"en\",trg_lang=\"rw\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4812c4-cbef-4326-b505-fd934dc2fc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forced_bos_token = \"kin_Latn\"\n",
    "model.config.forced_bos_token_id = tokenizer.lang_code_to_id[forced_bos_token]\n",
    "tokenizer.tgt_lang = 'kin_Latn'\n",
    "max_source_length = max_target_length = 128\n",
    "padding = \"max_length\"\n",
    "truncation = True\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "src_lang = \"eng_Latn\"\n",
    "trg_lang = \"kin_Latn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a2bbe-fa12-443b-a41c-e70ebf71b1da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = '/home/kabanda/data/train/en_kin_train.tsv'\n",
    "val_dir = '/home/kabanda/data/val/en_kin_val.tsv'\n",
    "test_dir = '/home/kabanda/data/test/en_kin_test.tsv'\n",
    "# train_dir = 'en_kin_train.tsv'\n",
    "# val_dir = 'en_kin_val.tsv'\n",
    "# test_dir = 'en_kin_test.tsv'\n",
    "file_dict = {'train': train_dir, 'validation': val_dir,'test': test_dir}\n",
    "dataset = load_dataset('csv',data_files = file_dict, sep=\"\\t\",encoding=\"cp1252\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb2fab-6e31-460b-be08-5a6f803a2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    #column_names: ['kin','en']\n",
    "    inputs = data[column_names[1]]\n",
    "    targets = data[column_names[0]]\n",
    "    model_inputs = tokenizer(inputs, max_length = max_source_length, padding  = padding, truncation = truncation)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length = max_target_length, padding = padding, truncation = truncation)\n",
    "    labels[\"input_ids\"] = [[(i if i != tokenizer.pad_token_id else -100) for i in label] for label in labels[\"input_ids\"]]\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531caf0-a266-472a-b248-47f837b32d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = dataset['train'].column_names\n",
    "train_dataset = dataset['train'].shuffle(seed=10)\n",
    "test_dataset = dataset['test'].shuffle(seed=10)\n",
    "val_dataset = dataset['validation'].shuffle(seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce92af-c390-44b8-9834-c76953b32b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocess, batched = True, remove_columns = column_names, desc = \"tokenizer train dataset\")\n",
    "val_dataset = val_dataset.map(preprocess, batched = True, remove_columns = column_names, desc = \"tokenizer val dataset\")\n",
    "test_dataset = test_dataset.map(preprocess, batched = True, remove_columns = column_names, desc = \"tokenizer test dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabdb8a-c6d2-4155-801f-86c8bf7333f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model, padding = padding) \n",
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04955a82-6a76-4b45-aa73-e7cbc257f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calc(data):\n",
    "    preds, true_labels = data\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens = True)\n",
    "    true_labels = np.where(true_labels != -100, true_labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(true_labels, skip_special_tokens = True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    result = metric.compute(predictions = decoded_preds, references = decoded_labels)\n",
    "    result = {\"bleu\":result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ed2b8-e6f0-4653-a939-700f0fd1e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_calc(data):\n",
    "    preds, true_labels = data\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens = True)\n",
    "    true_labels = np.where(true_labels != -100, true_labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(true_labels, skip_special_tokens = True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    result = metric.compute(predictions = decoded_preds, references = decoded_labels)\n",
    "    spm_result = metric.compute(predictions = decoded_preds, references = decoded_labels,tokenize='spm')\n",
    "    chrf_metric = load_metric(\"chrf\")\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds,references=decoded_labels,word_order=2)\n",
    "    ter_metric = load_metric(\"ter\")\n",
    "    ter_result = ter_metric.compute(predictions = decoded_preds,references = decoded_labels)\n",
    "    result = {\"bleu\":result[\"score\"],\"spbleu\":spm_result['score'],'ter':ter_result['score'],'chrf++':chrf_result['score']}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23264c24-fa67-4a2d-b4db-d7631f618ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='nllb_results_trial1',\n",
    "    num_train_epochs=epochs,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = 10000,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    load_best_model_at_end=True,\n",
    "    predict_with_generate=True,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251df27-ec94-4f38-a489-2a66c2c0633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=metrics_calc,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e960f7-92d8-4226-8b0d-15c16f01adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb20e7-b8a5-48fa-b1e8-a49dabd047da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
